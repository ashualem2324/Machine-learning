{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51382047",
   "metadata": {},
   "source": [
    "#                                            Import Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c7f4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ashua\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: langdetect in c:\\users\\ashua\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\ashua\\anaconda3\\lib\\site-packages (2022.7.9)\n",
      "Requirement already satisfied: obsei in c:\\users\\ashua\\anaconda3\\lib\\site-packages (0.0.14)\n",
      "Requirement already satisfied: dateparser in c:\\users\\ashua\\anaconda3\\lib\\site-packages (1.1.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.3 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from obsei) (4.12.2)\n",
      "Requirement already satisfied: mmh3>=3.0.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from obsei) (4.0.1)\n",
      "Requirement already satisfied: pydantic>=1.10.2 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from obsei) (1.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from obsei) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2022.6 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from obsei) (2023.3.post1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from obsei) (2.31.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.44 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from obsei) (2.0.22)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from dateparser) (2022.7.9)\n",
      "Requirement already satisfied: tzlocal in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from dateparser) (5.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.9.3->obsei) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from pydantic>=1.10.2->obsei) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->obsei) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from requests>=2.26.0->obsei) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from requests>=2.26.0->obsei) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from requests>=2.26.0->obsei) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from requests>=2.26.0->obsei) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.44->obsei) (2.0.1)\n",
      "Requirement already satisfied: tzdata in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tzlocal->dateparser) (2023.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\ashua\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\ashua\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\ashua\\anaconda3\\lib\\site-packages (2.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tensorflow-intel==2.14.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.24.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ashua\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas langdetect\n",
    "!pip install regex\n",
    "!pip install obsei dateparser\n",
    "!pip install pandas numpy tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a76a960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ashua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from langdetect import detect\n",
    "import emoji\n",
    "import warnings\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eac81a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel URL</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.youtube.com/channel/UCPox5oO3ubH_TQ...</td>\n",
       "      <td>I think this is the last straw that break the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.youtube.com/channel/UCxhvzuxbwK0Ncn...</td>\n",
       "      <td>The US needs to learn from China to become a w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.youtube.com/channel/UCf5kNNhD3jE8F3...</td>\n",
       "      <td>Thanks for the old news</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.youtube.com/channel/UCECVGJCqYazz4d...</td>\n",
       "      <td>Joe Biden and his administration Finance this ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.youtube.com/channel/UCSGosVOwjsSu5N...</td>\n",
       "      <td>news sung by Vanshika Jaral, with beautiful pr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Channel URL  \\\n",
       "0  http://www.youtube.com/channel/UCPox5oO3ubH_TQ...   \n",
       "1  http://www.youtube.com/channel/UCxhvzuxbwK0Ncn...   \n",
       "2  http://www.youtube.com/channel/UCf5kNNhD3jE8F3...   \n",
       "3  http://www.youtube.com/channel/UCECVGJCqYazz4d...   \n",
       "4  http://www.youtube.com/channel/UCSGosVOwjsSu5N...   \n",
       "\n",
       "                                             Comment  Category  \n",
       "0  I think this is the last straw that break the ...       NaN  \n",
       "1  The US needs to learn from China to become a w...       NaN  \n",
       "2                            Thanks for the old news       NaN  \n",
       "3  Joe Biden and his administration Finance this ...       NaN  \n",
       "4  news sung by Vanshika Jaral, with beautiful pr...       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"Youtubecomment.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c1658a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45172 entries, 0 to 45171\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Channel URL  28534 non-null  object \n",
      " 1   Comment      28534 non-null  object \n",
      " 2   Category     0 non-null      float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3b4bc",
   "metadata": {},
   "source": [
    "# preprocessing of YouTube Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1a519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove non-English text\n",
    "def remove_non_english(text):\n",
    "    try:\n",
    "        if detect(text) == 'en':\n",
    "            return text\n",
    "        else:\n",
    "            return ''\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    # Removes punctuation and replaces it with a space\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "# Apply the cleaning function to the 'Text' column\n",
    "data['Comment'] = data['Comment'].apply(remove_non_english)\n",
    "data['Comment'] = data['Comment'].apply(remove_punctuation)\n",
    "\n",
    "# Remove rows with empty or whitespace-only text\n",
    "data = data[data['Comment'].str.strip() != '']\n",
    "data.to_csv('clean_data.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67bb7184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Channel URL</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://www.youtube.com/channel/UCPox5oO3ubH_TQ...</td>\n",
       "      <td>I think this is the last straw that break the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://www.youtube.com/channel/UCxhvzuxbwK0Ncn...</td>\n",
       "      <td>The US needs to learn from China to become a w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.youtube.com/channel/UCf5kNNhD3jE8F3...</td>\n",
       "      <td>Thanks for the old news</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://www.youtube.com/channel/UCECVGJCqYazz4d...</td>\n",
       "      <td>Joe Biden and his administration Finance this ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.youtube.com/channel/UCSGosVOwjsSu5N...</td>\n",
       "      <td>news sung by Vanshika Jaral  with beautiful pr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21894</th>\n",
       "      <td>45066</td>\n",
       "      <td>http://www.youtube.com/channel/UCnT3-g1tO2r9V_...</td>\n",
       "      <td>Coward army</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21895</th>\n",
       "      <td>45069</td>\n",
       "      <td>http://www.youtube.com/channel/UCDB0yM3ffYdFj1...</td>\n",
       "      <td>Innocent people should never be punished for r...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21896</th>\n",
       "      <td>45161</td>\n",
       "      <td>http://www.youtube.com/channel/UCsYMcOEmOyhcSF...</td>\n",
       "      <td>We condemn Israel  39 s fascist  racist  bigot...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21897</th>\n",
       "      <td>45163</td>\n",
       "      <td>http://www.youtube.com/channel/UC_pfrX7p3c_f2S...</td>\n",
       "      <td>It  39 s so heart touching scene i am overwhel...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21898</th>\n",
       "      <td>45171</td>\n",
       "      <td>http://www.youtube.com/channel/UC1ZK7JyHinKg2m...</td>\n",
       "      <td>This is how terrorist pigs are hunted  a delig...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21899 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                        Channel URL  \\\n",
       "0               0  http://www.youtube.com/channel/UCPox5oO3ubH_TQ...   \n",
       "1               1  http://www.youtube.com/channel/UCxhvzuxbwK0Ncn...   \n",
       "2               2  http://www.youtube.com/channel/UCf5kNNhD3jE8F3...   \n",
       "3               3  http://www.youtube.com/channel/UCECVGJCqYazz4d...   \n",
       "4               4  http://www.youtube.com/channel/UCSGosVOwjsSu5N...   \n",
       "...           ...                                                ...   \n",
       "21894       45066  http://www.youtube.com/channel/UCnT3-g1tO2r9V_...   \n",
       "21895       45069  http://www.youtube.com/channel/UCDB0yM3ffYdFj1...   \n",
       "21896       45161  http://www.youtube.com/channel/UCsYMcOEmOyhcSF...   \n",
       "21897       45163  http://www.youtube.com/channel/UC_pfrX7p3c_f2S...   \n",
       "21898       45171  http://www.youtube.com/channel/UC1ZK7JyHinKg2m...   \n",
       "\n",
       "                                                 Comment  Category  \n",
       "0      I think this is the last straw that break the ...       NaN  \n",
       "1      The US needs to learn from China to become a w...       NaN  \n",
       "2                                Thanks for the old news       NaN  \n",
       "3      Joe Biden and his administration Finance this ...       NaN  \n",
       "4      news sung by Vanshika Jaral  with beautiful pr...       NaN  \n",
       "...                                                  ...       ...  \n",
       "21894                                      Coward army         NaN  \n",
       "21895  Innocent people should never be punished for r...       NaN  \n",
       "21896  We condemn Israel  39 s fascist  racist  bigot...       NaN  \n",
       "21897  It  39 s so heart touching scene i am overwhel...       NaN  \n",
       "21898  This is how terrorist pigs are hunted  a delig...       NaN  \n",
       "\n",
       "[21899 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataclean=pd.read_csv('clean_data.csv')\n",
    "dataclean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd9d74",
   "metadata": {},
   "source": [
    "# Data Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0f8ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ashua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Channel URL</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Category</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://www.youtube.com/channel/UCPox5oO3ubH_TQ...</td>\n",
       "      <td>I think this is the last straw that break the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://www.youtube.com/channel/UCxhvzuxbwK0Ncn...</td>\n",
       "      <td>The US needs to learn from China to become a w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.641</td>\n",
       "      <td>-0.4885</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.youtube.com/channel/UCf5kNNhD3jE8F3...</td>\n",
       "      <td>Thanks for the old news</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://www.youtube.com/channel/UCECVGJCqYazz4d...</td>\n",
       "      <td>Joe Biden and his administration Finance this ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.703</td>\n",
       "      <td>-0.6920</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.youtube.com/channel/UCSGosVOwjsSu5N...</td>\n",
       "      <td>news sung by Vanshika Jaral  with beautiful pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        Channel URL  \\\n",
       "0           0  http://www.youtube.com/channel/UCPox5oO3ubH_TQ...   \n",
       "1           1  http://www.youtube.com/channel/UCxhvzuxbwK0Ncn...   \n",
       "2           2  http://www.youtube.com/channel/UCf5kNNhD3jE8F3...   \n",
       "3           3  http://www.youtube.com/channel/UCECVGJCqYazz4d...   \n",
       "4           4  http://www.youtube.com/channel/UCSGosVOwjsSu5N...   \n",
       "\n",
       "                                             Comment  Category  Positive  \\\n",
       "0  I think this is the last straw that break the ...       NaN     0.000   \n",
       "1  The US needs to learn from China to become a w...       NaN     0.150   \n",
       "2                            Thanks for the old news       NaN     0.420   \n",
       "3  Joe Biden and his administration Finance this ...       NaN     0.061   \n",
       "4  news sung by Vanshika Jaral  with beautiful pr...       NaN     0.245   \n",
       "\n",
       "   Negative  Neutral  Compound Sentiment  \n",
       "0     0.000    1.000    0.0000   Neutral  \n",
       "1     0.208    0.641   -0.4885  Negative  \n",
       "2     0.000    0.580    0.4404  Positive  \n",
       "3     0.236    0.703   -0.6920  Negative  \n",
       "4     0.000    0.755    0.5994  Positive  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "dataclean[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in dataclean[\"Comment\"]]\n",
    "dataclean[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in dataclean[\"Comment\"]]\n",
    "dataclean[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in dataclean[\"Comment\"]]\n",
    "dataclean['Compound'] = [sentiments.polarity_scores(i)[\"compound\"] for i in dataclean[\"Comment\"]]\n",
    "score = dataclean[\"Compound\"].values\n",
    "sentiment = []\n",
    "for i in score:\n",
    "    if i >= 0.05 :\n",
    "        sentiment.append('Positive')\n",
    "    elif i <= -0.05 :\n",
    "        sentiment.append('Negative')\n",
    "    else:\n",
    "        sentiment.append('Neutral')\n",
    "dataclean[\"Sentiment\"] = sentiment\n",
    "dataclean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94022233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Channel URL</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://www.youtube.com/channel/UCPox5oO3ubH_TQ...</td>\n",
       "      <td>I think this is the last straw that break the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://www.youtube.com/channel/UCxhvzuxbwK0Ncn...</td>\n",
       "      <td>The US needs to learn from China to become a w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.youtube.com/channel/UCf5kNNhD3jE8F3...</td>\n",
       "      <td>Thanks for the old news</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://www.youtube.com/channel/UCECVGJCqYazz4d...</td>\n",
       "      <td>Joe Biden and his administration Finance this ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.youtube.com/channel/UCSGosVOwjsSu5N...</td>\n",
       "      <td>news sung by Vanshika Jaral  with beautiful pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        Channel URL  \\\n",
       "0           0  http://www.youtube.com/channel/UCPox5oO3ubH_TQ...   \n",
       "1           1  http://www.youtube.com/channel/UCxhvzuxbwK0Ncn...   \n",
       "2           2  http://www.youtube.com/channel/UCf5kNNhD3jE8F3...   \n",
       "3           3  http://www.youtube.com/channel/UCECVGJCqYazz4d...   \n",
       "4           4  http://www.youtube.com/channel/UCSGosVOwjsSu5N...   \n",
       "\n",
       "                                             Comment  Category Sentiment  \n",
       "0  I think this is the last straw that break the ...       NaN   Neutral  \n",
       "1  The US needs to learn from China to become a w...       NaN  Negative  \n",
       "2                            Thanks for the old news       NaN  Positive  \n",
       "3  Joe Biden and his administration Finance this ...       NaN  Negative  \n",
       "4  news sung by Vanshika Jaral  with beautiful pr...       NaN  Positive  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafinal=dataclean.drop(['Positive','Negative','Neutral','Compound'],axis=1)\n",
    "datafinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b9a82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafinal1=datafinal.drop(['Unnamed: 0','Category'],axis=1)\n",
    "datafinal1.to_csv('final_data.csv',index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1179b116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel URL</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.youtube.com/channel/UCPox5oO3ubH_TQ...</td>\n",
       "      <td>I think this is the last straw that break the ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.youtube.com/channel/UCxhvzuxbwK0Ncn...</td>\n",
       "      <td>The US needs to learn from China to become a w...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.youtube.com/channel/UCf5kNNhD3jE8F3...</td>\n",
       "      <td>Thanks for the old news</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.youtube.com/channel/UCECVGJCqYazz4d...</td>\n",
       "      <td>Joe Biden and his administration Finance this ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.youtube.com/channel/UCSGosVOwjsSu5N...</td>\n",
       "      <td>news sung by Vanshika Jaral  with beautiful pr...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Channel URL  \\\n",
       "0  http://www.youtube.com/channel/UCPox5oO3ubH_TQ...   \n",
       "1  http://www.youtube.com/channel/UCxhvzuxbwK0Ncn...   \n",
       "2  http://www.youtube.com/channel/UCf5kNNhD3jE8F3...   \n",
       "3  http://www.youtube.com/channel/UCECVGJCqYazz4d...   \n",
       "4  http://www.youtube.com/channel/UCSGosVOwjsSu5N...   \n",
       "\n",
       "                                             Comment Sentiment  \n",
       "0  I think this is the last straw that break the ...   Neutral  \n",
       "1  The US needs to learn from China to become a w...  Negative  \n",
       "2                            Thanks for the old news  Positive  \n",
       "3  Joe Biden and his administration Finance this ...  Negative  \n",
       "4  news sung by Vanshika Jaral  with beautiful pr...  Positive  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafinal1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b22d4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21899 entries, 0 to 21898\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Channel URL  21899 non-null  object\n",
      " 1   Comment      21899 non-null  object\n",
      " 2   Sentiment    21899 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 513.4+ KB\n"
     ]
    }
   ],
   "source": [
    "datafinal1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805a065",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c2d0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer() \n",
    "snowball_stemer = SnowballStemmer(language=\"english\")\n",
    "lzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dfee1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text):   \n",
    "    # convert text into lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove new line characters in text\n",
    "    text = re.sub(r'\\n',' ', text)\n",
    "    \n",
    "    # remove multiple spaces from text\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "    text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])\n",
    "    \n",
    "    # lemmatizer using WordNetLemmatizer from nltk package\n",
    "    text=' '.join([lzr.lemmatize(word) for word in word_tokenize(text)])\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_repeated_words(text):\n",
    "    # Use regular expressions to find and remove repeated words\n",
    "    return re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text)\n",
    "datafinal1['Comment'] = datafinal1['Comment'].apply(remove_repeated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74879672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "data_copy = datafinal1.copy()\n",
    "data_copy.Comment = data_copy.Comment.apply(lambda text: text_processing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b2c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "data_copy['Sentiment'] = le.fit_transform(data_copy['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d73ed18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>think last straw break camel back hamas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u need learn china become war free global econ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thanks old news</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joe biden administration finance war israel 39...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news sung vanshika jaral beautiful professiona...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Sentiment\n",
       "0            think last straw break camel back hamas          1\n",
       "1  u need learn china become war free global econ...          0\n",
       "2                                    thanks old news          2\n",
       "3  joe biden administration finance war israel 39...          0\n",
       "4  news sung vanshika jaral beautiful professiona...          2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = {\n",
    "    'Sentence':data_copy.Comment,\n",
    "    'Sentiment':data_copy['Sentiment']\n",
    "}\n",
    "\n",
    "processed_data = pd.DataFrame(processed_data)\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b964fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "0    11033\n",
       "2     6832\n",
       "1     4034\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6e7cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_neutral = processed_data[(processed_data['Sentiment']==1)] \n",
    "df_negative = processed_data[(processed_data['Sentiment']==0)]\n",
    "df_positive = processed_data[(processed_data['Sentiment']==2)]\n",
    "\n",
    "# upsample minority classes\n",
    "df_positive_upsampled = resample(df_positive, \n",
    "                                 replace=True,    \n",
    "                                 n_samples= 10000, \n",
    "                                 random_state=42)  \n",
    "\n",
    "df_neutral_upsampled = resample(df_neutral, \n",
    "                                 replace=True,    \n",
    "                                 n_samples= 10000, \n",
    "                                 random_state=42)  \n",
    "\n",
    "\n",
    "# Concatenate the upsampled dataframes with the neutral dataframe\n",
    "final_data2 = pd.concat([df_positive_upsampled,df_neutral_upsampled,df_negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd0731ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "0    11033\n",
       "2    10000\n",
       "1    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data2['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ec3ef326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pray everyday isreal palestine innocent civilian deserve treated like men keeping war going never frontlines',\n",
       " 'united state stand palestine people',\n",
       " 'thank cnn report pray israel',\n",
       " 'may allah swt protect brother sister palestine ameen',\n",
       " 'quot music festival quot holy land holy people quot exalt god quot']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for sentence in final_data2 ['Sentence']:\n",
    "    corpus.append(sentence)\n",
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43f14896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data2.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fc886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a5bc6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(final_data2['Sentence'])\n",
    "y = final_data2['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fbf3563d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {color: black;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a3aee886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy : 0.8259486022718118\n"
     ]
    }
   ],
   "source": [
    "print(\"training accuracy :\",model.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5a820a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e4dc8120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.70      0.86      0.77      2162\n",
      "    negative       0.84      0.73      0.79      2053\n",
      "    positive       0.82      0.73      0.78      1992\n",
      "\n",
      "    accuracy                           0.78      6207\n",
      "   macro avg       0.79      0.77      0.78      6207\n",
      "weighted avg       0.79      0.78      0.78      6207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['neutral', 'negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9249cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(final_data2['Sentence'])\n",
    "y = final_data2['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d66f1836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 0.86\n",
      "Fold 2 - Accuracy: 0.85\n",
      "Fold 3 - Accuracy: 0.85\n",
      "Fold 4 - Accuracy: 0.86\n",
      "Fold 5 - Accuracy: 0.85\n",
      "Mean Accuracy: 0.85\n",
      "Standard Deviation: 0.01\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "k = 5  # Set the number of folds\n",
    "scores = cross_val_score(model, X_train, y_train, cv=k, scoring='accuracy')\n",
    "# Print the cross-validation results\n",
    "for fold, score in enumerate(scores, start=1):\n",
    "    print(f'Fold {fold} - Accuracy: {score:.2f  }')\n",
    "mean_accuracy = scores.mean()\n",
    "std_deviation = scores.std()\n",
    "print(f'Mean Accuracy: {mean_accuracy:.2f}')\n",
    "print(f'Standard Deviation: {std_deviation:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4bd3f9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Predicted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>pray everyday isreal palestine innocent civili...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18442</th>\n",
       "      <td>united state stand palestine people</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17924</th>\n",
       "      <td>thank cnn report pray israel</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17816</th>\n",
       "      <td>may allah swt protect brother sister palestine...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13124</th>\n",
       "      <td>quot music festival quot holy land holy people...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21884</th>\n",
       "      <td>israel action br palestine suicidal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>haitian living horror 3 year intense terrorist...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21894</th>\n",
       "      <td>coward army</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21896</th>\n",
       "      <td>condemn israel 39 fascist racist bigotry barba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21898</th>\n",
       "      <td>terrorist pig hunted delight eye ear crusader ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31033 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  Sentiment  \\\n",
       "2388   pray everyday isreal palestine innocent civili...          2   \n",
       "18442                united state stand palestine people          2   \n",
       "17924                       thank cnn report pray israel          2   \n",
       "17816  may allah swt protect brother sister palestine...          2   \n",
       "13124  quot music festival quot holy land holy people...          2   \n",
       "...                                                  ...        ...   \n",
       "21884                israel action br palestine suicidal          0   \n",
       "21890  haitian living horror 3 year intense terrorist...          0   \n",
       "21894                                        coward army          0   \n",
       "21896  condemn israel 39 fascist racist bigotry barba...          0   \n",
       "21898  terrorist pig hunted delight eye ear crusader ...          0   \n",
       "\n",
       "       Predicted Sentiment  \n",
       "2388                     2  \n",
       "18442                    2  \n",
       "17924                    2  \n",
       "17816                    2  \n",
       "13124                    2  \n",
       "...                    ...  \n",
       "21884                    1  \n",
       "21890                    0  \n",
       "21894                    1  \n",
       "21896                    0  \n",
       "21898                    0  \n",
       "\n",
       "[31033 rows x 3 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "final_data2['Predicted Sentiment'] = model.predict(X)\n",
    "final_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5a7da379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.88      0.83      0.85      2162\n",
      "    negative       0.85      0.93      0.89      2053\n",
      "    positive       0.88      0.85      0.87      1992\n",
      "\n",
      "    accuracy                           0.87      6207\n",
      "   macro avg       0.87      0.87      0.87      6207\n",
      "weighted avg       0.87      0.87      0.87      6207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['neutral', 'negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "05bffe52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "00769124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 0.90\n",
      "Fold 2 - Accuracy: 0.90\n",
      "Fold 3 - Accuracy: 0.90\n",
      "Fold 4 - Accuracy: 0.90\n",
      "Fold 5 - Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Perform K-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "k = 5  # Set the number of folds\n",
    "scores = cross_val_score(random_forest, X_train, y_train, cv=k, scoring='accuracy')\n",
    "# Print the cross-validation results\n",
    "for fold, score in enumerate(scores, start=1):\n",
    "    print(f'Fold {fold} - Accuracy: {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "051bdfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "y_pred_rand = random_forest.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred_rand)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bb825cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.91      0.87      0.89      2162\n",
      "    negative       0.91      0.97      0.94      2053\n",
      "    positive       0.93      0.90      0.92      1992\n",
      "\n",
      "    accuracy                           0.92      6207\n",
      "   macro avg       0.92      0.92      0.92      6207\n",
      "weighted avg       0.92      0.92      0.91      6207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_rand, target_names=['neutral', 'negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6967d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
